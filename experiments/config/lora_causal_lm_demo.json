{
	"pretrained_model": "EleutherAI/polyglot-ko-1.3b",
	"input_template": "{} {}",
	"lora_r": 8,
	"lora_alpha": 16,
	"lora_dropout": 0.1,
	"max_length" : 128,
	"per_device_batch_size": 8,
	"epochs": 10,
	"gradient_accumulation_steps": 1,
	"dropout": 0.1,
	"label_smoothing_factor": 0.1,
	"learning_rate": 5e-6,
	"warmup_proportion": 0.1,
	"adam_epsilon": 1e-8,
	"warmup_steps": 0,
	"max_grad_norm": 1,
	"logging_steps": 10,
	"save_strategy": "steps",
	"save_steps": 10,
	"save_total_limit": 1,
	"evaluation_strategy": "steps",
	"summary_step": 10,
	"evaluate_during_training": true,
	"metric_for_best_model": "eval_loss",
	"seed": 100,
	"fp16": true,
	"bf16": false
  }